{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Jupyter notebook is used to create HazardAhead and other Tiger Graph Analytics sample data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample data created in this notebook does not use any proprietary/license data, however, most of the examples/samples generated use all available free data samples from internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames, CSV, Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"galaxy.csv\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "galaxy = DataFrame(\n",
    "    unit = \"Sun\",\n",
    "    star = true,\n",
    "    gas=true,\n",
    "    dust=true,\n",
    "    breathableAir=false;\n",
    "    water=false,\n",
    "    land=false,\n",
    "    sky=false\n",
    "    )\n",
    "push!(galaxy, [\"Earth\", false, true, true, true, true, true, true])\n",
    "push!(galaxy, [\"Jupiter\", false, true, true, false, false, true, true])\n",
    "push!(galaxy, [\"Saturn\", false, true, true, false, false, true, true])\n",
    "push!(galaxy, [\"Mars\", false, true, true, false, false, true, true])\n",
    "push!(galaxy, [\"Mercury\", false, true, true, false, false, true, true])\n",
    "push!(galaxy, [\"Uranus\", false, true, true, false, false, true, true])\n",
    "push!(galaxy, [\"Venus\", false, true, true, false, false, true, true])\n",
    "\n",
    "CSV.write(\"galaxy.csv\", galaxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"species.csv\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species = DataFrame(\n",
    "    unit = \"Dinosaur\",\n",
    "    intelligence = true,\n",
    "    crawl = 7,\n",
    "    swim = 0,\n",
    "    walk = 7,\n",
    "    runningSpeed = 7,\n",
    "    eat = 9,\n",
    "    drink = 3,\n",
    "    sleep = 6\n",
    "    )\n",
    "push!(species, [\"Mammoth\", true, 9, 3, 6, 5, 8, 2, 8])\n",
    "push!(species, [\"Birds\", true, 0, 0, 0, 9, 1, 1, 2])\n",
    "push!(species, [\"BigFoot\", true, 7, 7, 7, 7, 8, 2, 8])\n",
    "push!(species, [\"Cockroach\", false, 0, 0, 0, 10, 1, 1, 1])\n",
    "push!(species, [\"Human\", false, 9, 1, 1, 1, 9, 10, 9])\n",
    "\n",
    "CSV.write(\"species.csv\", species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"itemmaster.csv\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemmaster = DataFrame(\n",
    "    unit = [\"ToiletPaper\",\n",
    "    \"Vacuum\",\"Bucket\",\"Rags or chamois mop\",\"dishwashing soap\",\"All-purpose cleaner\",\"Scrubbing sponges\",\"Baking soda\",\"White vinegar\",\"lemons\"\n",
    "    ],\n",
    "    drug = false,\n",
    "    category = \"Cleaning Supplies\",\n",
    "    essentials = true,\n",
    "    luxury = false\n",
    "    )\n",
    "push!(itemmaster, [\"Adenosine\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Atropine \",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Calcium gluconate\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Dextrose 10%\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Dopamine\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Epinephrine 1 : 10,000\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Fentanyl\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Hydralazine\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Lorazepam\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Morphine\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Naloxone\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Phenobarbital\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Sodium Bicarbonate\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Vecuronium\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"Volume Expanders RBCs, NS\",true,\"Rx\",true,false])\n",
    "\n",
    "push!(itemmaster, [\"sodium nitrite\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"sodium nitroprusside\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"sodium thiosulfate\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"sodium valproate\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"spectinomycin\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"spironolactone\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"stavudine\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"streptokinase\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"streptomycin\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"sulfadiazine\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"sulfadoxine\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"sulfadoxine + pyrimethamine\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"sulfamethoxazole\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"trimethoprim\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"tamoxifen\",true,\"Rx\",true,false])\n",
    "push!(itemmaster, [\"testosterone\",true,\"Rx\",true,false])\n",
    "\n",
    "CSV.write(\"itemmaster.csv\", itemmaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"livein.csv\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "livein = DataFrame(\n",
    "    From = \"Human\",\n",
    "    To = \"Earth\",\n",
    "    living_since = Date(\"1000-01-01\", dateformat\"y-m-d\")\n",
    "    )\n",
    "push!(livein, [\"Mammoth\", \"Earth\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Birds\", \"Earth\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"BigFoot\", \"Earth\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Cockroach\", \"Earth\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Human\", \"Earth\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Dinosaur\", \"Earth\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Dinosaur\", \"Venus\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Dinosaur\", \"Mercury\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Cockroaches\", \"Mars\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Cockroaches\", \"Jupiter\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Cockroaches\", \"Saturn\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Cockroaches\", \"Mercury\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Cockroaches\", \"Uranus\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(livein, [\"Cockroaches\", \"Venus\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "\n",
    "CSV.write(\"livein.csv\", livein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"behavior.csv\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior = DataFrame(\n",
    "    From = \"Human\",\n",
    "    To = \"Mammoth\",\n",
    "    like_date = Date(\"1000-01-01\", dateformat\"y-m-d\")\n",
    "    )\n",
    "push!(behavior, [\"Human\", \"Birds\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "push!(behavior, [\"Human\", \"Human\", Date(\"0050-01-01\", dateformat\"y-m-d\")])\n",
    "\n",
    "CSV.write(\"behavior.csv\", behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"give_food.csv\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "give_food = DataFrame(\n",
    "    From = \"Earth\",\n",
    "    To = \"Human\"\n",
    "    )\n",
    "push!(give_food, [\"Earth\", \"Birds\"])\n",
    "push!(give_food, [\"Earth\", \"Cockroaches\"])\n",
    "\n",
    "CSV.write(\"give_food.csv\", give_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"essentials.csv\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essentials = DataFrame(\n",
    "    From = \"Human\",\n",
    "    To = \"Birds\"\n",
    "    )\n",
    "push!(essentials, [\"Human\", \"Cockroaches\"])\n",
    "\n",
    "CSV.write(\"essentials.csv\", essentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"usedby.csv\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    used_by = DataFrame(\n",
    "        From = \"ToiletPaper\",\n",
    "        To = \"Human\"\n",
    "        )\n",
    "    push!(used_by, [\"Baking soda\", \"Human\"])\n",
    "    push!(used_by, [\"Dopamine\", \"Human\"])\n",
    "    push!(used_by, [\"Morphine\", \"Human\"])\n",
    "    push!(used_by, [\"All-purpose cleaner\", \"Human\"])\n",
    "\n",
    "    CSV.write(\"usedby.csv\", used_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2P Finance Graph Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"accounts.csv\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accountsDF = DataFrame(\n",
    "    ENTITY = \"Apple Inc.\",\n",
    "    AS_OF_DATE=Date(\"1900-01-01\", dateformat\"y-m-d\"),\n",
    "    ID = 11000:1000:45000,\n",
    "    CLASSIFICATION=repeat([\n",
    "        \"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"\n",
    "                ], inner=5),\n",
    "    CATEGORY=[\n",
    "        \"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n",
    "        \"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n",
    "        \"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n",
    "        \"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n",
    "        \"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n",
    "        \"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n",
    "        \"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"],\n",
    "    STATUS=\"A\",\n",
    "    DESCR=repeat([\n",
    "    \"operating expenses\",\"non-operating expenses\",\"assets\",\"liability\",\"net-worth\",\"stats\",\"revenue\"], inner=5),\n",
    "    ACCOUNT_TYPE=repeat([\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"],inner=5));\n",
    "\n",
    "    # DEPARTMENT Chartfield\n",
    "deptDF = DataFrame(\n",
    "    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n",
    "    ID = 1100:100:1500,\n",
    "    CLASSIFICATION=[\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"],\n",
    "    CATEGORY=[\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"],\n",
    "    STATUS=\"A\",\n",
    "    DESCR=[\n",
    "    \"Sales & Marketing\",\"Human Resource\",\"Infomration Technology\",\"Business leaders\",\"other temp\"\n",
    "        ],\n",
    "    DEPT_TYPE=[\"S\",\"H\",\"I\",\"B\",\"O\"]);\n",
    "\n",
    "# LOCATION Chartfield\n",
    "locationDF = DataFrame(\n",
    "    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n",
    "    ID = 11:1:22,\n",
    "    CLASSIFICATION=repeat([\n",
    "        \"Region A\",\"Region B\", \"Region C\"], inner=4),\n",
    "    CATEGORY=repeat([\n",
    "        \"Region A\",\"Region B\", \"Region C\"], inner=4),\n",
    "    STATUS=\"A\",\n",
    "    DESCR=[\n",
    "\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\",\n",
    "\"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\",\n",
    "\"Dallas\",\"San Francisco\"],\n",
    "    LOC_TYPE=\"Physical\");\n",
    "    # CSV.write(\"accounts.csv\", accountsDF)\n",
    "\n",
    "    # creating Ledger\n",
    "ledgerDF = DataFrame(\n",
    "    LEDGER = String[], FISCAL_YEAR = Int[], PERIOD = Int[], ORGID = String[],\n",
    "    OPER_UNIT = String[], ACCOUNT = Int[], DEPT = Int[], LOCATION = Int[],\n",
    "    POSTED_TOTAL = Float64[]\n",
    "    );\n",
    "\n",
    "# create 2020 Period 1-12 Actuals Ledger \n",
    "l = \"Actuals\";\n",
    "fy = 2020;\n",
    "for p = 1:12\n",
    "for i = 1:10^5\n",
    "push!(ledgerDF, (l, fy, p, \"ABC Inc.\", rand(locationDF.CATEGORY),\n",
    "    rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n",
    "end\n",
    "end\n",
    "\n",
    "# create 2021 Period 1-4 Actuals Ledger \n",
    "l = \"Actuals\";\n",
    "fy = 2021;\n",
    "for p = 1:4\n",
    "for i = 1:10^5\n",
    "push!(ledgerDF, (l, fy, p, \"ABC Inc.\", rand(locationDF.CATEGORY),\n",
    "    rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n",
    "end\n",
    "end\n",
    "\n",
    "# create 2021 Period 1-4 Budget Ledger \n",
    "l = \"Budget\";\n",
    "fy = 2021;\n",
    "for p = 1:12\n",
    "for i = 1:10^5\n",
    "push!(ledgerDF, (l, fy, p, \"ABC Inc.\", rand(locationDF.CATEGORY),\n",
    "    rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n",
    "end\n",
    "end\n",
    "\n",
    "# here is ~3 million rows ledger dataframe\n",
    "CSV.write(\"accounts.csv\", accountsDF)\n",
    "CSV.write(\"locations.csv\", locationDF)\n",
    "CSV.write(\"department.csv\", deptDF)\n",
    "CSV.write(\"ledger.csv\", ledgerDF[1:1000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P2P Supply Chain Graph Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.webio.node+json": {
       "children": [],
       "instanceArgs": {
        "namespace": "html",
        "tag": "div"
       },
       "nodeType": "DOM",
       "props": {},
       "type": "node"
      },
      "text/html": [
       "<div style=\"padding: 1em; background-color: #f8d6da; border: 1px solid #f5c6cb; font-weight: bold;\">\n",
       "<p>The WebIO Jupyter extension was not detected. See the\n",
       "<a href=\"https://juliagizmos.github.io/WebIO.jl/latest/providers/ijulia/\" target=\"_blank\">\n",
       "    WebIO Jupyter integration documentation\n",
       "</a>\n",
       "for more information.\n",
       "</div>\n"
      ],
      "text/plain": [
       "WebIO._IJuliaInit()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## create SUPPLY CHAIN DATA ###\n",
    "###############################\n",
    "# Item master, Item Attribs, Item Costing ##\n",
    "#       UNSPSC, GUDID, GTIN, GMDN\n",
    "############################################\n",
    "\n",
    "##########\n",
    "# UNSPSC #\n",
    "##########\n",
    "# UNSPSC file can be downloaded from this link https://www.ungm.org/Public/UNSPSC\n",
    "xf = XLSX.readxlsx(\"sampleData/UNGM_UNSPSC_12-Apr-2022.xlsx\")\n",
    "# xf will display names of sheets and rows with data\n",
    "# let's read this data in to a DataFrame\n",
    "\n",
    "# using below command will read xlsx data into DataFrame but will not render column labels\n",
    "# df = DataFrame(XLSX.readdata(\"UNGM_UNSPSC_09-Apr-2022..xlsx\", \"UNSPSC\", \"A1:D12988\"), :auto)\n",
    "dfUNSPSC = DataFrame(XLSX.readtable(\"sampleData/UNGM_UNSPSC_09-Apr-2022..xlsx\", \"UNSPSC\")...)\n",
    "# ... operator will splat the tuple (data, column_labels) into the constructor of DataFrame\n",
    "\n",
    "# replace missing values with an integer 99999\n",
    "replace!(dfUNSPSC.\"Parent key\", missing => 99999)\n",
    "size(dfUNSPSC)\n",
    "\n",
    "# let's export this clean csv, we'll load this into database\n",
    "CSV.write(\"UNSPSC.csv\", dfUNSPSC)\n",
    "\n",
    "# # remember to empty dataFrame after usage\n",
    "# # Julia will flush it out automatically after session,\n",
    "# # but often ERP data gets bulky during session\n",
    "# Base.summarysize(dfUNSPSC)\n",
    "# empty!(dfUNSPSC)\n",
    "# Base.summarysize(dfUNSPSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########\n",
    "# GUDID ##\n",
    "##########\n",
    "# The complete list of GUDID Data Elements and descriptions can be found at this link.\n",
    "# https://www.fda.gov/media/120974/download\n",
    "# The complete GUDID Database (delimited version) download (250+MB)\n",
    "# https://accessgudid.nlm.nih.gov/release_files/download/AccessGUDID_Delimited_Full_Release_20220401.zip\n",
    "# let's extract all GUDID files in a folder\n",
    "# readdir(pwd())\n",
    "# readdir(\"sampleData/GUDID\")\n",
    "# since these files are in txt (delimited) format, we'll use delimited pkg\n",
    "\n",
    "########################\n",
    "## large txt files #####\n",
    "## read one at a time ##\n",
    "########################\n",
    "\n",
    "# data, header = readdlm(\"sampleData/GUDID/contacts.txt\", '|', header=true)\n",
    "# dfGUDIDcontacts = DataFrame(data, vec(header))\n",
    "\n",
    "# data, header = readdlm(\"sampleData/GUDID/identifiers.txt\", '|', header=true)\n",
    "# dfGUDIDidentifiers = DataFrame(data, vec(header))\n",
    "\n",
    "data, header = readdlm(\"sampleData/GUDID/device.txt\", '|', header=true)\n",
    "dfGUDIDdevice = DataFrame(data, vec(header))\n",
    "\n",
    "# # remember to empty dataFrame after usage\n",
    "# # Julia will flush it out automatically after session,\n",
    "# # but often ERP data gets bulky during session\n",
    "# Base.summarysize(dfGUDIDcontacts),Base.summarysize(dfGUDIDidentifiers),Base.summarysize(dfGUDIDdevice)\n",
    "# empty!(dfGUDIDcontacts)\n",
    "# empty!(dfGUDIDidentifiers)\n",
    "# empty!(dfGUDIDdevice)\n",
    "# Base.summarysize(dfGUDIDcontacts),Base.summarysize(dfGUDIDidentifiers),Base.summarysize(dfGUDIDdevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfGUDIDdevice has more than 3308327 rows,\n",
    "# let's split this in 6 mini files, \n",
    "# so that, it can be loaded into RDBMS easily\n",
    "size(dfGUDIDdevice)\n",
    "# CSV.write(\"dfGUDIDdevice_1.csv\", dfGUDIDdevice[1:500000,:])\n",
    "# CSV.write(\"dfGUDIDdevice_2.csv\", dfGUDIDdevice[500001:1000000,:])\n",
    "# CSV.write(\"dfGUDIDdevice_3.csv\", dfGUDIDdevice[1000001:1500000,:])\n",
    "# CSV.write(\"dfGUDIDdevice_4.csv\", dfGUDIDdevice[1500001:2000000,:])\n",
    "# CSV.write(\"dfGUDIDdevice_5.csv\", dfGUDIDdevice[2000001:2500000,:])\n",
    "# CSV.write(\"dfGUDIDdevice_6.csv\", dfGUDIDdevice[2500001:3308327,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# GTIN ###\n",
    "##########\n",
    "\n",
    "# xf = XLSX.readxlsx(\"SampleData/DS_GTIN_ALL.xlsx\")\n",
    "# xf will display names of sheets and rows with data\n",
    "# let's read this data in to a DataFrame\n",
    "\n",
    "# using below command will read xlsx data into DataFrame but will not render column labels\n",
    "# df = DataFrame(XLSX.readdata(\"SampleData/DS_GTIN_ALL.xlsx\", \"Worksheet\", \"A14:E143403   \"), :auto)\n",
    "dfGTIN = DataFrame(XLSX.readtable(\"sampleData/DS_GTIN_ALL.xlsx\", \"Worksheet\";first_row=14)...)\n",
    "# ... operator will splat the tuple (data, column_labels) into the constructor of DataFrame\n",
    "\n",
    "# replace missing values with an integer 99999\n",
    "# replace!(dfUNSPSC.\"Parent key\", missing => 99999)\n",
    "# size(dfUNSPSC)\n",
    "\n",
    "# let's export this clean csv, we'll load this into database\n",
    "# CSV.write(\"UNSPSC.csv\", dfUNSPSC)\n",
    "# readdir(pwd())\n",
    "\n",
    "# # remember to empty dataFrame after usage\n",
    "# # Julia will flush it out automatically after session,\n",
    "# # but often ERP data gets bulky during session\n",
    "# Base.summarysize(dfGTIN)\n",
    "# empty!(dfGTIN)\n",
    "# Base.summarysize(dfGTIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# GMDN ###\n",
    "##########\n",
    "\n",
    "## GMDN data is not available\n",
    "\n",
    "# # remember to empty dataFrame after usage\n",
    "# # Julia will flush it out automatically after session,\n",
    "# # but often ERP data gets bulky during session\n",
    "# Base.summarysize(dfGMDN)\n",
    "# empty!(dfGMDN)\n",
    "# Base.summarysize(dfGMDN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vendor Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Vendor master #\n",
    "#################\n",
    "# create Vendor Master from GUDID dataset\n",
    "# show(first(dfGUDIDdevice,5), allcols=true)\n",
    "# show(first(dfGUDIDdevice[:,[:brandName, :catalogNumber, :dunsNumber, :companyName, :rx, :otc]],5), allcols=true)\n",
    "# names(dfGUDIDdevice)\n",
    "# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :catalogNumber, :dunsNumber, :companyName, :rx, :otc]])\n",
    "# dfVendor = unique(dfGUDIDdevice[:,[:companyName]]) # 7574 unique vendors\n",
    "dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n",
    "# dfVendor is a good dataset, have 216k rows for 7574 unique vendors\n",
    "\n",
    "# # remember to empty dataFrame after usage\n",
    "# # Julia will flush it out automatically after session,\n",
    "# # but often ERP data gets bulky during session\n",
    "# Base.summarysize(dfVendor)\n",
    "# empty!(dfVendor)\n",
    "# Base.summarysize(dfVendor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, header = readdlm(\"sampleData/uscities.csv\", ',', header=true)\n",
    "dfLocation = DataFrame(data, vec(header))\n",
    "\n",
    "# # remember to empty dataFrame after usage\n",
    "# # Julia will flush it out automatically after session,\n",
    "# # but often ERP data gets bulky during session\n",
    "# Base.summarysize(dfLocation)\n",
    "# empty!(dfLocation)\n",
    "# Base.summarysize(dfLocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organization Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOrgMaster = DataFrame(\n",
    "    ENTITY=repeat([\"HeadOffice\"], inner=8),\n",
    "    GROUP=repeat([\"Operations\"], inner=8),\n",
    "    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n",
    "    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MSR Material Service Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize = 1000 # number of rows, scale as needed\n",
    "\n",
    "dfMSR = DataFrame(\n",
    "    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n",
    "    MSR_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n",
    "    FROM_UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n",
    "    TO_UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n",
    "    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n",
    "    QTY = rand(dfOrgMaster.UNIT, sampleSize));\n",
    "first(dfMSR, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purchase Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize = 1000 # number of rows, scale as needed\n",
    "\n",
    "dfPO = DataFrame(\n",
    "    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n",
    "    PO_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n",
    "    VENDOR=rand(unique(dfVendor.companyName), sampleSize),\n",
    "    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n",
    "    QTY = rand(1:150, sampleSize),\n",
    "    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n",
    "    );\n",
    "show(first(dfPO, 5),allcols=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voucher Invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize = 1000 # number of rows, scale as needed\n",
    "\n",
    "dfVCHR = DataFrame(\n",
    "    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n",
    "    VCHR_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n",
    "    STATUS=rand([\"Closed\",\"Paid\",\"Open\",\"Cancelled\",\"Exception\"], sampleSize),\n",
    "    VENDOR_INVOICE_NUM = rand(10001:9999999, sampleSize),\n",
    "    VENDOR=rand(unique(dfVendor.companyName), sampleSize),\n",
    "    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n",
    "    QTY = rand(1:150, sampleSize),\n",
    "    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n",
    "    );\n",
    "show(first(dfVCHR, 5),allcols=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize = 1000 # number of rows, scale as needed\n",
    "\n",
    "dfREVENUE = DataFrame(\n",
    "    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n",
    "    SALES_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n",
    "    STATUS=rand([\"Sold\",\"Pending\",\"Hold\",\"Cancelled\",\"Exception\"], sampleSize),\n",
    "    SALES_RECEIPT_NUM = rand(10001:9999999, sampleSize),\n",
    "    CUSTOMER=rand(unique(dfVendor.companyName), sampleSize),\n",
    "    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n",
    "    QTY = rand(1:150, sampleSize),\n",
    "    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n",
    "    );\n",
    "show(first(dfREVENUE, 5),allcols=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shipment Receipt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSize = 1000 # number of rows, scale as needed\n",
    "\n",
    "dfSHIPRECEIPT = DataFrame(\n",
    "    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n",
    "    SHIP_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n",
    "    STATUS=rand([\"Shippped\",\"Returned\",\"In process\",\"Cancelled\",\"Exception\"], sampleSize),\n",
    "    SHIPMENT_NUM = rand(10001:9999999, sampleSize),\n",
    "    CUSTOMER=rand(unique(dfVendor.companyName), sampleSize),\n",
    "    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n",
    "    QTY = rand(1:150, sampleSize),\n",
    "    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n",
    "    );\n",
    "show(first(dfSHIPRECEIPT, 5),allcols=true)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Julia (4 threads) 1.7.0",
   "language": "julia",
   "name": "julia-(4-threads)-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
